import re
import os
import time
from datetime import datetime, timedelta
from openai import OpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed


# é…ç½®APIå®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-qckbdjlxsfwejnhghzimuwleeuvnvvvsqxvfcbmujknlsopm",
    base_url="https://api.siliconflow.cn/v1",
)

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("test_model_answer", exist_ok=True)


def load_questions(file_path="mil_questions.txt"):
    """ä»æ–‡ä»¶åŠ è½½é—®é¢˜"""
    questions = []
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # åˆ†ç¦»æ¯ä¸ªé—®é¢˜
    q_pattern = r'Q\d+:(.*?)(?=Q\d+:|$)'
    matches = re.findall(q_pattern, content, re.DOTALL)

    # æ¸…ç†å¹¶æ·»åŠ é—®é¢˜
    for q in matches:
        questions.append(q.strip())

    return questions


def get_model_answer(question, model_name, question_num=0):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹å›ç­”é—®é¢˜"""
    system_prompt = """
    æ‚¨æ˜¯å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜ï¼Œç»“åˆé€»è¾‘æ¨ç†ã€æŠ€æœ¯è§„èŒƒå’Œå†›äº‹æ ‡å‡†ï¼Œæä¾›è¯¦ç»†ä¸”å‡†ç¡®çš„å›ç­”ã€‚  
    è¦æ±‚ç­”æ¡ˆç²¾ç»ƒç®€æ´ï¼Œå‡†ç¡®ç‡é«˜ï¼Œä½“ç°å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š  
    1. **æ“ä½œæ­¥éª¤**ï¼šæ¸…æ™°çš„æ‰§è¡Œæµç¨‹ã€‚  
    2. **æŠ€æœ¯è§„èŒƒ**ï¼šç›¸å…³æŠ€æœ¯æ ‡å‡†æˆ–æ–¹æ³•ï¼Œä¸ç”¨åŒ…å«æ•°å­—ã€‚  
    3. **å†›äº‹æ ‡å‡†**ï¼šé€‚ç”¨çš„å†›äº‹è§„èŒƒæˆ–è¦æ±‚ã€‚  
    4. **é€»è¾‘æ¨ç†**ï¼šå±•ç¤ºé—®é¢˜åˆ†æã€è§£å†³æ–¹æ¡ˆæ¨å¯¼çš„è¿‡ç¨‹ã€‚  
    
    **è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š**  
    Q1: [é—®é¢˜æ­£æ–‡]  
    A1: [ç­”æ¡ˆæ­£æ–‡ï¼ŒåŒ…å«æ“ä½œæ­¥éª¤ã€æŠ€æœ¯è§„èŒƒã€å†›äº‹æ ‡å‡†åŠé€»è¾‘æ¨ç†è¿‡ç¨‹]  
    Q2: [é—®é¢˜æ­£æ–‡]  
    A2: [ç­”æ¡ˆæ­£æ–‡ï¼ŒåŒ…å«æ“ä½œæ­¥éª¤ã€æŠ€æœ¯è§„èŒƒã€å†›äº‹æ ‡å‡†åŠé€»è¾‘æ¨ç†è¿‡ç¨‹]  
    ...  
    
    **è¯·ç¡®ä¿å›ç­”ï¼š**  
    1. **ç®€æ˜æ‰¼è¦**ï¼šé¿å…å†—ä½™ä¿¡æ¯ï¼Œçªå‡ºé‡ç‚¹ã€‚  
    2. **å‡†ç¡®æ— è¯¯**ï¼šç¬¦åˆæŠ€æœ¯è§„èŒƒå’Œå†›äº‹æ ‡å‡†ã€‚  
    3. **é€»è¾‘æ¸…æ™°**ï¼šå±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼Œä½“ç°é—®é¢˜è§£å†³èƒ½åŠ›ã€‚  
    4. **å¤šæ ·åŒ–**ï¼šé’ˆå¯¹ä¸åŒç±»å‹é—®é¢˜ï¼Œæä¾›é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆã€‚  
    
    ---
    """

    start_time = time.time()
    try:
        params = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }


        response = client.chat.completions.create(**params)
        elapsed_time = time.time() - start_time
        return {
            "num": question_num,
            "answer": response.choices[0].message.content,
            "time": elapsed_time
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        return {
            "num": question_num,
            "answer": f"æ¨¡å‹å›ç­”å‡ºé”™: {str(e)}",
            "time": elapsed_time
        }


def process_questions_with_model(model_name, model_display_name=None, max_workers=4):
    """å¤„ç†æ‰€æœ‰é—®é¢˜å¹¶ä¿å­˜æ¨¡å‹å›ç­”ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ"""
    if model_display_name is None:
        model_display_name = model_name

    questions = load_questions()
    output_file = f"test_model_answer/{model_display_name}_answer.txt"

    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹ {model_display_name} è¿›è¡Œé—®é¢˜å›ç­”...")
    print(f"ğŸ§µ å¯ç”¨å¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {max_workers}")

    # è®°å½•æ•´ä½“å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†é—®é¢˜
    answers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_question = {
            executor.submit(get_model_answer, question, model_name, idx + 1): idx
            for idx, question in enumerate(questions)
        }

        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(total=len(questions), desc="æ¨¡å‹å›ç­”è¿›åº¦", unit="é—®é¢˜")

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_question):
            result = future.result()
            answers.append(result)
            progress_bar.update(1)

        progress_bar.close()

    # è®°å½•æ•´ä½“ç»“æŸæ—¶é—´å’Œæ€»è€—æ—¶
    total_end_time = time.time()
    total_elapsed_time = total_end_time - total_start_time
    total_elapsed_formatted = str(timedelta(seconds=round(total_elapsed_time)))

    # æŒ‰é—®é¢˜é¡ºåºæ’åºç­”æ¡ˆ
    answers.sort(key=lambda x: x["num"])

    # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
    response_times = [a["time"] for a in answers]
    avg_response_time = sum(response_times) / len(response_times)

    # å†™å…¥æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        # å†™å…¥å¤´éƒ¨ä¿¡æ¯
        f.write(f"# æ¨¡å‹ {model_display_name} å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é—®é¢˜å›ç­”\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n")

        # å†™å…¥ç­”æ¡ˆï¼ˆä¸åŒ…å«é—®é¢˜ï¼‰
        for answer_data in answers:
            f.write(f"{answer_data['answer']}\n")
            f.write(f"[å“åº”æ—¶é—´: {answer_data['time']:.2f}ç§’]\n\n")
            f.write("-" * 80 + "\n\n")

        # å†™å…¥æ—¶é—´ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶æœ€å
        f.write("=" * 80 + "\n")
        f.write("æ—¶é—´ç»Ÿè®¡\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ€»å¤„ç†æ—¶é—´: {total_elapsed_formatted} (æ€»è®¡: {total_elapsed_time:.2f}ç§’)\n")
        f.write(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’\n")
        f.write(f"æœ€å¿«å“åº”: {min(response_times):.2f}ç§’\n")
        f.write(f"æœ€æ…¢å“åº”: {max(response_times):.2f}ç§’\n")
        f.write("=" * 80 + "\n")

    print(f"\nâœ… æ¨¡å‹å›ç­”å·²ä¿å­˜è‡³: {output_file}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed_formatted}")
    return output_file


def run_multiple_models(model_config):
    """è¿è¡Œå¤šä¸ªæ¨¡å‹æµ‹è¯•"""
    print("ğŸ›¡ï¸ å¯åŠ¨GPUåŠ é€Ÿçš„å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é—®é¢˜æ¨¡å‹è¯„æµ‹ç¨‹åº")
    total_evaluation_start = time.time()

    results = []
    for model in model_config:
        max_workers = 8
        output_file = process_questions_with_model(model["id"], model["name"], max_workers)
        results.append({
            "model": model["name"],
            "output_file": output_file
        })

    total_evaluation_time = time.time() - total_evaluation_start
    total_time_formatted = str(timedelta(seconds=round(total_evaluation_time)))

    print("\nğŸ“Š è¯„æµ‹å®Œæˆæ¦‚è§ˆ:")
    for result in results:
        print(f"- {result['model']}: {result['output_file']}")
    print(f"\nâ±ï¸ å…¨éƒ¨è¯„æµ‹æ€»è€—æ—¶: {total_time_formatted}")


if __name__ == "__main__":
    try:
        # é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹
        models_to_test = [

            {
                "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                "name": "DeepSeek-R1-Distill-Qwen-7B",
            },
            {
                "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "name": "DeepSeek-R1-Distill-Qwen-32B",
            },
            {
                "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
                "name": "DeepSeek-R1-Distill-Qwen-14B",
            },
            {
                "id": "Qwen/Qwen2.5-32B-Instruct",
                "name": "Qwen2.5-32B-Instruct",
            },
            {
                "id": "Qwen/Qwen2.5-14B-Instruct",
                "name": "Qwen2.5-14B-Instruct",
            },
            {
                "id": "Qwen/Qwen2.5-7B-Instruct",
                "name": "Qwen2.5-7B-Instruct",
            },
            {
                "id": "Qwen/QwQ-32B-Preview",
                "name": "QwQ-32B-Preview",
            },
            {
                "id": "THUDM/glm-4-9b-chat",
                "name": "glm-4-9b-chat",
            },
            {
                "id": "deepseek-ai/DeepSeek-R1",
                "name": "deepseek-R1",
            }
        ]

        # è¿è¡Œå¤šæ¨¡å‹è¯„æµ‹
        run_multiple_models(models_to_test)

    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå‘Šè­¦ï¼š{str(e)}")
        # è®°å½•é”™è¯¯æ—¥å¿—
        with open("combat_system_error.log", "a") as log:
            log.write(f"[{datetime.now()}] {str(e)}\n")

