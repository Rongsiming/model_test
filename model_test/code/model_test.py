import re
import os
import time
from datetime import datetime, timedelta
from openai import OpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed


# é…ç½®APIå®¢æˆ·ç«¯
client = OpenAI(
    api_key="22e0a307-fcf4-4029-b697-d42cde4b1733",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
)

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("test_model_answer", exist_ok=True)


def load_questions(file_path="mil_questions.txt"):
    """ä»æ–‡ä»¶åŠ è½½é—®é¢˜"""
    questions = []
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # åˆ†ç¦»æ¯ä¸ªé—®é¢˜
    q_pattern = r'Q\d+:(.*?)(?=Q\d+:|$)'
    matches = re.findall(q_pattern, content, re.DOTALL)

    # æ¸…ç†å¹¶æ·»åŠ é—®é¢˜
    for q in matches:
        questions.append(q.strip())

    return questions


def get_model_answer(question, model_name, question_num=0):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹å›ç­”é—®é¢˜"""
    system_prompt = """
    æ‚¨æ˜¯å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·è¯¦ç»†å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
    ç­”æ¡ˆå¿…é¡»åŒ…å«å…·ä½“å‚æ•°ï¼ˆå¦‚æ¸©åº¦èŒƒå›´ï¼š-40â„ƒ~71â„ƒï¼ŒåŠ å¯†æ ‡å‡†ï¼šAES-256ï¼‰ã€‚
    è¦æ±‚ç­”æ¡ˆç²¾ç®€ç®€çŸ­ï¼Œå‡†ç¡®ç‡é«˜ï¼ŒåŒ…æ‹¬å…·ä½“çš„æ“ä½œæ­¥éª¤ã€æŠ€æœ¯è§„èŒƒå’Œå†›äº‹æ ‡å‡†è¦æ±‚ã€‚
    éœ€è¦é‡å¤é—®é¢˜å†…å®¹ã€‚
    """

    start_time = time.time()
    try:
        params = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "temperature": 0.5,
            "max_tokens": 1500
        }


        response = client.chat.completions.create(**params)
        elapsed_time = time.time() - start_time
        return {
            "num": question_num,
            "answer": response.choices[0].message.content,
            "time": elapsed_time
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        return {
            "num": question_num,
            "answer": f"æ¨¡å‹å›ç­”å‡ºé”™: {str(e)}",
            "time": elapsed_time
        }


def process_questions_with_model(model_name, model_display_name=None, max_workers=4):
    """å¤„ç†æ‰€æœ‰é—®é¢˜å¹¶ä¿å­˜æ¨¡å‹å›ç­”ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ"""
    if model_display_name is None:
        model_display_name = model_name

    questions = load_questions()
    output_file = f"test_model_answer/{model_display_name}_answer.txt"

    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹ {model_display_name} è¿›è¡Œé—®é¢˜å›ç­”...")
    print(f"ğŸ§µ å¯ç”¨å¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {max_workers}")

    # è®°å½•æ•´ä½“å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†é—®é¢˜
    answers = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_question = {
            executor.submit(get_model_answer, question, model_name, idx + 1): idx
            for idx, question in enumerate(questions)
        }

        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(total=len(questions), desc="æ¨¡å‹å›ç­”è¿›åº¦", unit="é—®é¢˜")

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_question):
            result = future.result()
            answers.append(result)
            progress_bar.update(1)

        progress_bar.close()

    # è®°å½•æ•´ä½“ç»“æŸæ—¶é—´å’Œæ€»è€—æ—¶
    total_end_time = time.time()
    total_elapsed_time = total_end_time - total_start_time
    total_elapsed_formatted = str(timedelta(seconds=round(total_elapsed_time)))

    # æŒ‰é—®é¢˜é¡ºåºæ’åºç­”æ¡ˆ
    answers.sort(key=lambda x: x["num"])

    # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
    response_times = [a["time"] for a in answers]
    avg_response_time = sum(response_times) / len(response_times)

    # å†™å…¥æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        # å†™å…¥å¤´éƒ¨ä¿¡æ¯
        f.write(f"# æ¨¡å‹ {model_display_name} å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é—®é¢˜å›ç­”\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n")

        # å†™å…¥ç­”æ¡ˆï¼ˆä¸åŒ…å«é—®é¢˜ï¼‰
        for answer_data in answers:
            f.write(f"A{answer_data['num']}: {answer_data['answer']}\n")
            f.write(f"[å“åº”æ—¶é—´: {answer_data['time']:.2f}ç§’]\n\n")
            f.write("-" * 80 + "\n\n")

        # å†™å…¥æ—¶é—´ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶æœ€å
        f.write("=" * 80 + "\n")
        f.write("æ—¶é—´ç»Ÿè®¡\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ€»å¤„ç†æ—¶é—´: {total_elapsed_formatted} (æ€»è®¡: {total_elapsed_time:.2f}ç§’)\n")
        f.write(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’\n")
        f.write(f"æœ€å¿«å“åº”: {min(response_times):.2f}ç§’\n")
        f.write(f"æœ€æ…¢å“åº”: {max(response_times):.2f}ç§’\n")
        f.write("=" * 80 + "\n")

    print(f"\nâœ… æ¨¡å‹å›ç­”å·²ä¿å­˜è‡³: {output_file}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_elapsed_formatted}")
    return output_file


def run_multiple_models(model_config):
    """è¿è¡Œå¤šä¸ªæ¨¡å‹æµ‹è¯•"""
    print("ğŸ›¡ï¸ å¯åŠ¨GPUåŠ é€Ÿçš„å†›äº‹æŒ‡æŒ¥ç³»ç»Ÿè¿ç»´é—®é¢˜æ¨¡å‹è¯„æµ‹ç¨‹åº")
    total_evaluation_start = time.time()

    results = []
    for model in model_config:
        max_workers = 8
        output_file = process_questions_with_model(model["id"], model["name"], max_workers)
        results.append({
            "model": model["name"],
            "output_file": output_file
        })

    total_evaluation_time = time.time() - total_evaluation_start
    total_time_formatted = str(timedelta(seconds=round(total_evaluation_time)))

    print("\nğŸ“Š è¯„æµ‹å®Œæˆæ¦‚è§ˆ:")
    for result in results:
        print(f"- {result['model']}: {result['output_file']}")
    print(f"\nâ±ï¸ å…¨éƒ¨è¯„æµ‹æ€»è€—æ—¶: {total_time_formatted}")


if __name__ == "__main__":
    try:
        # é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹
        models_to_test = [
            {
                "id": "ep-20250227212251-6zq2v",  # APIè°ƒç”¨æ—¶ä½¿ç”¨çš„ID
                "name": "DeepSeek-R1-Distill-Qwen-32B"  # æ˜¾ç¤ºåç§°å’Œæ–‡ä»¶å
            },

            {
                "id": "ep-20250218175517-6jc9n",
                "name": "deepseek-R1",
            }
        ]

        # è¿è¡Œå¤šæ¨¡å‹è¯„æµ‹
        run_multiple_models(models_to_test)

    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå‘Šè­¦ï¼š{str(e)}")
        # è®°å½•é”™è¯¯æ—¥å¿—
        with open("combat_system_error.log", "a") as log:
            log.write(f"[{datetime.now()}] {str(e)}\n")

